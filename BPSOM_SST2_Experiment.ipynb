{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BP-SOM for BERT Fine-tuning on SST-2\n",
    "\n",
    "This notebook implements BP-SOM (Back-Propagation with Self-Organizing Maps) for BERT fine-tuning on the SST-2 sentiment analysis task.\n",
    "\n",
    "**BP-SOM** combines supervised backpropagation with unsupervised SOM clustering:\n",
    "- Standard BP error from task loss\n",
    "- SOM error from class-specific clustering\n",
    "- Combined learning: `error = (1-α) * bp_error + α * som_error`\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run this notebook on a GPU-enabled environment (Colab, Kaggle, etc.) for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch transformers datasets numpy scipy scikit-learn matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertModel,\n",
    "    BertConfig,\n",
    "    BertPreTrainedModel,\n",
    "    BertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "\n",
    "# Check GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Self-Organizing Map (SOM) Layer\n",
    "\n",
    "Implements the core SOM algorithm for BP-SOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SelfOrganizingMap(nn.Module):\n    \"\"\"\n    Self-Organizing Map for BP-SOM architecture.\n    Optimized with vectorized operations for GPU acceleration.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        grid_size: int,\n        num_classes: int,\n        som_lr_max: float = 0.20,\n        som_lr_min: float = 0.05,\n        som_context_max: int = 2,\n        som_context_min: int = 0,\n        reliability_threshold: float = 0.95,\n        som_error_weight: float = 0.25,\n    ):\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.grid_size = grid_size\n        self.num_classes = num_classes\n        self.som_lr_max = som_lr_max\n        self.som_lr_min = som_lr_min\n        self.som_context_max = som_context_max\n        self.som_context_min = som_context_min\n        self.reliability_threshold = reliability_threshold\n        self.som_error_weight = som_error_weight\n\n        # SOM grid storage\n        self.register_buffer(\n            'som_vectors',\n            torch.rand(grid_size, grid_size, input_dim) * 0.5 + 0.5\n        )\n        self.register_buffer(\n            'cell_class_counts',\n            torch.zeros(grid_size, grid_size, num_classes + 1, dtype=torch.long)\n        )\n        self.register_buffer(\n            'cell_labels',\n            torch.zeros(grid_size, grid_size, dtype=torch.long)\n        )\n        self.register_buffer(\n            'cell_reliability',\n            torch.zeros(grid_size, grid_size, dtype=torch.float)\n        )\n\n        self.total_distance = 0.0\n        self.som_usage_count = 0\n        self.total_examples = 0\n\n    def get_distance_matrix(self, x: torch.Tensor, w: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Compute squared Euclidean distance matrix between batch x and vectors w.\n        x: (B, D), w: (N, D) -> returns (B, N)\n        \"\"\"\n        x_norm = (x**2).sum(1).view(-1, 1)\n        w_norm = (w**2).sum(1).view(1, -1)\n        dist = x_norm + w_norm - 2.0 * torch.mm(x, w.t())\n        return torch.clamp(dist, 0.0, None)\n\n    def update_som_network_batch(\n        self,\n        activations: torch.Tensor,\n        bmu_indices: torch.Tensor,\n        som_lr: float,\n        som_context: int\n    ):\n        \"\"\"Update SOM prototypes using batch neighborhood learning.\"\"\"\n        B = activations.size(0)\n        G = self.grid_size\n\n        # Convert indices to coordinates\n        bmu_x = bmu_indices // G\n        bmu_y = bmu_indices % G\n\n        # Create coordinate grid (G, G)\n        y_grid, x_grid = torch.meshgrid(\n            torch.arange(G, device=activations.device),\n            torch.arange(G, device=activations.device),\n            indexing='ij'\n        )\n\n        # Compute Chebyshev distances for all grid cells to all BMUs\n        # Expand to (B, G, G)\n        dist_x = torch.abs(x_grid.unsqueeze(0) - bmu_x.view(B, 1, 1))\n        dist_y = torch.abs(y_grid.unsqueeze(0) - bmu_y.view(B, 1, 1))\n        dist_chebyshev = torch.max(dist_x, dist_y)\n\n        # Compute update powers\n        mask = (dist_chebyshev <= som_context).float()\n        powers = som_lr / (2.0 ** dist_chebyshev)\n        powers = powers * mask # (B, G, G)\n\n        # Accumulate updates: sum_k (p_k * (x_k - w))\n        # W_new = W_old + sum_k(p_k * x_k) - W_old * sum_k(p_k)\n\n        denom = powers.sum(dim=0).unsqueeze(-1) # (G, G, 1)\n\n        powers_flat = powers.view(B, -1) # (B, G*G)\n        num_flat = torch.matmul(powers_flat.t(), activations) # (G*G, B) x (B, D) -> (G*G, D)\n        numerator = num_flat.view(G, G, -1)\n\n        self.som_vectors += (numerator - self.som_vectors * denom)\n\n    def forward(\n        self,\n        activations: torch.Tensor,\n        labels: Optional[torch.Tensor] = None,\n        epoch: int = 0,\n        max_epochs: int = 100,\n        training: bool = True\n    ) -> Tuple[Optional[torch.Tensor], dict]:\n        \"\"\"Process activations through SOM using vectorized operations.\"\"\"\n        batch_size = activations.size(0)\n\n        # Compute current SOM parameters\n        progress = (max_epochs - epoch) / max_epochs if max_epochs > 0 else 0\n        progress = max(0, min(1, progress))\n\n        som_context = self.som_context_min + int(\n            (progress ** 4) * (self.som_context_max - self.som_context_min)\n        )\n        som_lr = self.som_lr_min + (progress ** 1) * (self.som_lr_max - self.som_lr_min)\n\n        # Flatten SOM vectors for distance computation: (G*G, D)\n        flat_som = self.som_vectors.view(-1, self.input_dim)\n\n        # 1. Compute Distances and Global BMUs\n        dists = self.get_distance_matrix(activations, flat_som) # (B, G*G)\n        min_dists, bmu_indices = torch.min(dists, dim=1) # (B,)\n\n        # Statistics\n        total_dist = torch.sqrt(min_dists).sum().item()\n        som_used_count = 0\n        som_errors = None\n\n        if training and labels is not None:\n            # 2. Compute Class-Specific BMUs and Errors\n            # Mask distances where cell_label != label\n            flat_labels = self.cell_labels.view(-1)\n            # (B, G*G) mask\n            label_mask = (flat_labels.unsqueeze(0) != labels.unsqueeze(1))\n\n            class_dists = dists.clone()\n            class_dists[label_mask] = float('inf')\n\n            _, class_bmu_indices = torch.min(class_dists, dim=1)\n\n            # Get prototypes and reliability for class BMUs\n            class_bmu_vectors = flat_som[class_bmu_indices] # (B, D)\n            flat_reliability = self.cell_reliability.view(-1)\n            reliabilities = flat_reliability[class_bmu_indices] / 100.0 # (B,)\n\n            # Compute SOM errors\n            # valid if reliability >= threshold\n            valid_mask = (reliabilities >= self.reliability_threshold).float().unsqueeze(1)\n            som_errors = 0.01 * reliabilities.unsqueeze(1) * (class_bmu_vectors - activations)\n            som_errors = som_errors * valid_mask\n\n            som_used_count = (reliabilities >= self.reliability_threshold).sum().item()\n\n            # 3. Update SOM Network\n            if min_dists.mean() > 1e-4:\n                self.update_som_network_batch(activations, bmu_indices, som_lr, som_context)\n\n        self.total_distance += total_dist\n        self.som_usage_count += som_used_count\n        self.total_examples += batch_size\n\n        stats = {\n            'som_lr': som_lr,\n            'som_context': som_context,\n            'avg_distance': total_dist / batch_size if batch_size > 0 else 0,\n            'som_usage_pct': (som_used_count / batch_size * 100) if batch_size > 0 else 0,\n        }\n\n        return som_errors, stats\n\n    def update_cell_labels(self, activations: torch.Tensor, labels: torch.Tensor):\n        \"\"\"Update SOM cell class labels using batched processing.\"\"\"\n        self.cell_class_counts.zero_()\n\n        flat_som = self.som_vectors.view(-1, self.input_dim)\n        batch_size = 1024 # Process in chunks to save memory\n\n        with torch.no_grad():\n            for i in range(0, activations.size(0), batch_size):\n                batch_act = activations[i:i+batch_size]\n                batch_lbl = labels[i:i+batch_size]\n\n                dists = self.get_distance_matrix(batch_act, flat_som)\n                _, bmu_indices = torch.min(dists, dim=1)\n\n                bmu_x = bmu_indices // self.grid_size\n                bmu_y = bmu_indices % self.grid_size\n\n                # Accumulate counts\n                for j, label in enumerate(batch_lbl):\n                    self.cell_class_counts[bmu_x[j], bmu_y[j], label] += 1\n\n        # Update labels and reliability\n        for x in range(self.grid_size):\n            for y in range(self.grid_size):\n                counts = self.cell_class_counts[x, y]\n                total = counts.sum().item()\n\n                if total > 0:\n                    max_count, max_class = counts.max(0)\n                    self.cell_labels[x, y] = max_class\n                    self.cell_reliability[x, y] = (max_count.float() / total) * 100\n                else:\n                    self.cell_labels[x, y] = 0\n                    self.cell_reliability[x, y] = 0\n\n    def reset_statistics(self):\n        \"\"\"Reset epoch statistics.\"\"\"\n        self.total_distance = 0.0\n        self.som_usage_count = 0\n        self.total_examples = 0\n\n    def get_statistics(self) -> dict:\n        \"\"\"Get accumulated statistics.\"\"\"\n        return {\n            'avg_distance': self.total_distance / max(1, self.total_examples),\n            'som_usage_pct': (self.som_usage_count / max(1, self.total_examples)) * 100,\n            'total_examples': self.total_examples,\n        }\n\nprint(\"✓ Vectorized SOM Layer defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BP-SOM BERT Model\n",
    "\n",
    "Integrates BERT with BP-SOM hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class BPSOMHiddenLayer(nn.Module):\n    \"\"\"Hidden layer with attached SOM for BP-SOM learning.\"\"\"\n\n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int,\n        num_classes: int,\n        dropout: float = 0.1,\n        som_grid_size: int = 20,\n        som_lr_max: float = 0.20,\n        som_lr_min: float = 0.05,\n        som_context_max: int = 2,\n        som_context_min: int = 0,\n        reliability_threshold: float = 0.95,\n        som_error_weight: float = 0.25,\n    ):\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n\n        self.weight = nn.Parameter(torch.randn(hidden_dim, input_dim) * 0.1)\n        self.bias = nn.Parameter(torch.zeros(hidden_dim))\n        self.activation = nn.Sigmoid()\n        self.dropout = nn.Dropout(dropout)\n\n        self.som = SelfOrganizingMap(\n            input_dim=hidden_dim,\n            grid_size=som_grid_size,\n            num_classes=num_classes,\n            som_lr_max=som_lr_max,\n            som_lr_min=som_lr_min,\n            som_context_max=som_context_max,\n            som_context_min=som_context_min,\n            reliability_threshold=reliability_threshold,\n            som_error_weight=som_error_weight,\n        )\n\n        self.som_error_weight = som_error_weight\n\n        self.register_buffer('activation_sum', torch.zeros(hidden_dim))\n        self.register_buffer('activation_sum_sq', torch.zeros(hidden_dim))\n        self.register_buffer('activation_count', torch.zeros(1))\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        labels: Optional[torch.Tensor] = None,\n        epoch: int = 0,\n        max_epochs: int = 100,\n        training: bool = True\n    ) -> Tuple[torch.Tensor, dict]:\n        \"\"\"Forward pass with SOM processing.\"\"\"\n        z = torch.nn.functional.linear(x, self.weight, self.bias)\n        activations = self.activation(z)\n\n        if training:\n            with torch.no_grad():\n                self.activation_sum += activations.sum(dim=0)\n                self.activation_sum_sq += (activations ** 2).sum(dim=0)\n                self.activation_count += activations.size(0)\n\n        som_errors, som_stats = self.som.forward(\n            activations,\n            labels=labels,\n            epoch=epoch,\n            max_epochs=max_epochs,\n            training=training\n        )\n\n        if training and labels is not None and som_errors is not None:\n            def som_gradient_hook(grad):\n                bp_weight = 1.0 - self.som_error_weight\n                combined_grad = bp_weight * grad + self.som_error_weight * som_errors\n                return combined_grad\n\n            activations.register_hook(som_gradient_hook)\n\n        activations = self.dropout(activations)\n        return activations, som_stats\n\n    def get_activation_statistics(self) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Get mean and std of activations.\"\"\"\n        if self.activation_count > 0:\n            mean = self.activation_sum / self.activation_count\n            variance = (self.activation_sum_sq / self.activation_count) - (mean ** 2)\n            std = torch.sqrt(torch.clamp(variance, min=0))\n        else:\n            mean = torch.zeros_like(self.activation_sum)\n            std = torch.zeros_like(self.activation_sum)\n        return mean, std\n\n    def reset_activation_statistics(self):\n        \"\"\"Reset activation tracking.\"\"\"\n        self.activation_sum.zero_()\n        self.activation_sum_sq.zero_()\n        self.activation_count.zero_()\n\n\nclass BPSOMBertForSequenceClassification(BertPreTrainedModel):\n    \"\"\"BERT model with BP-SOM hidden layer for sequence classification.\"\"\"\n\n    def __init__(self, config, bpsom_config=None):\n        super().__init__(config)\n\n        self.num_labels = config.num_labels\n        self.config = config\n\n        self.bert = BertModel(config)\n\n        if bpsom_config is None:\n            bpsom_config = {\n                'hidden_size': 128,\n                'dropout': 0.1,\n                'som_grid_size': 20,\n                'som_lr_max': 0.20,\n                'som_lr_min': 0.05,\n                'som_context_max': 2,\n                'som_context_min': 0,\n                'reliability_threshold': 0.95,\n                'som_error_weight': 0.25,\n            }\n\n        self.bpsom_hidden = BPSOMHiddenLayer(\n            input_dim=config.hidden_size,\n            hidden_dim=bpsom_config.get('hidden_size', 128),\n            num_classes=config.num_labels,\n            dropout=bpsom_config.get('dropout', 0.1),\n            som_grid_size=bpsom_config.get('som_grid_size', 20),\n            som_lr_max=bpsom_config.get('som_lr_max', 0.20),\n            som_lr_min=bpsom_config.get('som_lr_min', 0.05),\n            som_context_max=bpsom_config.get('som_context_max', 2),\n            som_context_min=bpsom_config.get('som_context_min', 0),\n            reliability_threshold=bpsom_config.get('reliability_threshold', 0.95),\n            som_error_weight=bpsom_config.get('som_error_weight', 0.25),\n        )\n\n        self.classifier = nn.Linear(bpsom_config.get('hidden_size', 128), config.num_labels)\n\n        self.post_init()\n\n        self.current_epoch = 0\n        self.max_epochs = 100\n\n    def set_epoch(self, epoch: int, max_epochs: int):\n        \"\"\"Set current epoch for SOM parameter scheduling.\"\"\"\n        self.current_epoch = epoch\n        self.max_epochs = max_epochs\n\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        labels: Optional[torch.Tensor] = None,\n        return_dict: Optional[bool] = None,\n    ) -> SequenceClassifierOutput:\n        \"\"\"Forward pass through BERT + BP-SOM + Classifier.\"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            return_dict=True,\n        )\n\n        cls_output = outputs.last_hidden_state[:, 0, :]\n\n        hidden_output, som_stats = self.bpsom_hidden(\n            cls_output,\n            labels=labels,\n            epoch=self.current_epoch,\n            max_epochs=self.max_epochs,\n            training=self.training\n        )\n\n        logits = self.classifier(hidden_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def update_som_labels(self, dataloader, device):\n        \"\"\"Update SOM cell labels based on training data.\"\"\"\n        all_activations = []\n        all_labels = []\n\n        self.eval()\n        with torch.no_grad():\n            for batch in tqdm(dataloader, desc=\"Updating SOM labels\"):\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n\n                outputs = self.bert(input_ids, attention_mask=attention_mask, return_dict=True)\n                cls_output = outputs.last_hidden_state[:, 0, :]\n\n                z = torch.nn.functional.linear(cls_output, self.bpsom_hidden.weight, self.bpsom_hidden.bias)\n                activations = self.bpsom_hidden.activation(z)\n\n                # Keep on device for GPU-accelerated SOM operations\n                all_activations.append(activations)\n                all_labels.append(labels)\n\n        all_activations = torch.cat(all_activations, dim=0)\n        all_labels = torch.cat(all_labels, dim=0)\n\n        self.bpsom_hidden.som.update_cell_labels(all_activations, all_labels)\n        self.train()\n\nprint(\"✓ BP-SOM BERT Model defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading\n",
    "\n",
    "Load SST-2 dataset from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load SST-2 dataset\n",
    "print(\"Loading SST-2 dataset...\")\n",
    "dataset = load_dataset('glue', 'sst2')\n",
    "\n",
    "print(f\"Train examples: {len(dataset['train'])}\")\n",
    "print(f\"Validation examples: {len(dataset['validation'])}\")\n",
    "print(f\"Test examples: {len(dataset['test'])}\")\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence'], padding='max_length', truncation=True, max_length=128)\n",
    "\n",
    "print(\"\\nTokenizing...\")\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Prepare dataloaders\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'input_ids': torch.tensor([item['input_ids'] for item in batch]),\n",
    "        'attention_mask': torch.tensor([item['attention_mask'] for item in batch]),\n",
    "        'labels': torch.tensor([item['label'] for item in batch])\n",
    "    }\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets['train'],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets['validation'],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Data loaded and prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_epoch(model, dataloader, optimizer, scheduler, device, epoch, max_epochs):\n    \"\"\"Train for one epoch.\"\"\"\n    model.train()\n    model.set_epoch(epoch, max_epochs)\n\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{max_epochs} [Train]\")\n\n    for batch in progress_bar:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        optimizer.zero_grad()\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        total_loss += loss.item()\n        predictions = torch.argmax(outputs.logits, dim=-1)\n        correct += (predictions == labels).sum().item()\n        total += labels.size(0)\n\n        progress_bar.set_postfix({\n            'loss': f\"{loss.item():.4f}\",\n            'acc': f\"{correct/total*100:.2f}%\"\n        })\n\n    return {\n        'loss': total_loss / len(dataloader),\n        'accuracy': correct / total * 100\n    }\n\n\ndef eval_epoch(model, dataloader, device, desc=\"Eval\"):\n    \"\"\"Evaluate model.\"\"\"\n    model.eval()\n\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=desc):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n\n            total_loss += outputs.loss.item()\n            predictions = torch.argmax(outputs.logits, dim=-1)\n            correct += (predictions == labels).sum().item()\n            total += labels.size(0)\n\n    return {\n        'loss': total_loss / len(dataloader),\n        'accuracy': correct / total * 100\n    }\n\nprint(\"✓ Training functions defined\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_som_visualization(model, class_names=['negative', 'positive']):\n",
    "    \"\"\"Plot SOM cell labels and reliability.\"\"\"\n",
    "    som = model.bpsom_hidden.som\n",
    "    labels = som.cell_labels.cpu().numpy()\n",
    "    reliability = som.cell_reliability.cpu().numpy()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "    \n",
    "    # Labels\n",
    "    sns.heatmap(\n",
    "        labels,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='tab20',\n",
    "        square=True,\n",
    "        cbar_kws={'label': 'Class'},\n",
    "        ax=axes[0]\n",
    "    )\n",
    "    axes[0].set_title('SOM Cell Labels', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('X')\n",
    "    axes[0].set_ylabel('Y')\n",
    "    \n",
    "    # Reliability\n",
    "    sns.heatmap(\n",
    "        reliability,\n",
    "        annot=True,\n",
    "        fmt='.0f',\n",
    "        cmap='YlOrRd',\n",
    "        vmin=0,\n",
    "        vmax=100,\n",
    "        square=True,\n",
    "        cbar_kws={'label': 'Reliability (%)'},\n",
    "        ax=axes[1]\n",
    "    )\n",
    "    axes[1].set_title('SOM Cell Reliability', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('X')\n",
    "    axes[1].set_ylabel('Y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_training_curves(history):\n",
    "    \"\"\"Plot training and validation curves.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss\n",
    "    axes[0].plot(history['train_loss'], label='Train', marker='o')\n",
    "    axes[0].plot(history['eval_loss'], label='Validation', marker='s')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Loss Curves')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    axes[1].plot(history['train_accuracy'], label='Train', marker='o')\n",
    "    axes[1].plot(history['eval_accuracy'], label='Validation', marker='s')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('Accuracy Curves')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_comparison(baseline_history, bpsom_history):\n",
    "    \"\"\"Plot comparison between baseline and BP-SOM.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Validation Loss\n",
    "    axes[0].plot(baseline_history['eval_loss'], label='Baseline BERT', marker='o', linewidth=2)\n",
    "    axes[0].plot(bpsom_history['eval_loss'], label='BP-SOM BERT', marker='s', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Validation Loss', fontsize=12)\n",
    "    axes[0].set_title('Comparison: Validation Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(fontsize=11)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Validation Accuracy\n",
    "    axes[1].plot(baseline_history['eval_accuracy'], label='Baseline BERT', marker='o', linewidth=2)\n",
    "    axes[1].plot(bpsom_history['eval_accuracy'], label='BP-SOM BERT', marker='s', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Validation Accuracy (%)', fontsize=12)\n",
    "    axes[1].set_title('Comparison: Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(fontsize=11)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ Visualization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Baseline BERT\n",
    "\n",
    "First, train a baseline BERT model for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nnum_epochs = 3\nlearning_rate = 2e-5\nwarmup_steps = 500\n\nprint(\"=\" * 80)\nprint(\"BASELINE BERT EXPERIMENT\")\nprint(\"=\" * 80)\n\n# Initialize baseline model\nbaseline_config = BertConfig.from_pretrained('bert-base-uncased')\nbaseline_config.num_labels = 2\n\nbaseline_model = BertForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    config=baseline_config\n).to(device)\n\n# Add a dummy set_epoch method to the baseline model to satisfy the train_epoch function's expectation.\n# The original BertForSequenceClassification does not have this method, but BPSOMBertForSequenceClassification does.\ndef dummy_set_epoch(self, epoch: int, max_epochs: int):\n    pass\nbaseline_model.set_epoch = dummy_set_epoch.__get__(baseline_model, type(baseline_model))\n\n\n# Optimizer and scheduler\nbaseline_optimizer = torch.optim.AdamW(baseline_model.parameters(), lr=learning_rate)\ntotal_steps = len(train_dataloader) * num_epochs\nbaseline_scheduler = get_linear_schedule_with_warmup(\n    baseline_optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)\n\n# Training loop\nbaseline_history = {\n    'train_loss': [],\n    'train_accuracy': [],\n    'eval_loss': [],\n    'eval_accuracy': []\n}\n\nbest_baseline_acc = 0.0\n\nfor epoch in range(num_epochs):\n    # Train\n    train_metrics = train_epoch(\n        baseline_model,\n        train_dataloader,\n        baseline_optimizer,\n        baseline_scheduler,\n        device,\n        epoch,\n        num_epochs\n    )\n\n    # Evaluate\n    eval_metrics = eval_epoch(baseline_model, eval_dataloader, device, \"Validation\")\n\n    # Log\n    print(f\"\\nEpoch {epoch+1}/{num_epochs}:\")\n    print(f\"  Train - Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['accuracy']:.2f}%\")\n    print(f\"  Val   - Loss: {eval_metrics['loss']:.4f}, Acc: {eval_metrics['accuracy']:.2f}%\")\n\n    baseline_history['train_loss'].append(train_metrics['loss'])\n    baseline_history['train_accuracy'].append(train_metrics['accuracy'])\n    baseline_history['eval_loss'].append(eval_metrics['loss'])\n    baseline_history['eval_accuracy'].append(eval_metrics['accuracy'])\n\n    if eval_metrics['accuracy'] > best_baseline_acc:\n        best_baseline_acc = eval_metrics['accuracy']\n        print(f\"  ✓ New best validation accuracy!\")\n\n    print(\"-\" * 80)\n\nprint(f\"\\nBaseline Training Complete!\")\nprint(f\"Best Validation Accuracy: {best_baseline_acc:.2f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot baseline results\n",
    "plot_training_curves(baseline_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train BP-SOM BERT\n",
    "\n",
    "Now train the BP-SOM enhanced model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 80)\nprint(\"BP-SOM BERT EXPERIMENT\")\nprint(\"=\" * 80)\n\n# Initialize BP-SOM model\nbpsom_config = BertConfig.from_pretrained('bert-base-uncased')\nbpsom_config.num_labels = 2\n\nbpsom_model_config = {\n    'hidden_size': 128,\n    'dropout': 0.1,\n    'som_grid_size': 20,\n    'som_error_weight': 0.25,\n    'som_lr_max': 0.20,\n    'som_lr_min': 0.05,\n    'som_context_max': 2,\n    'som_context_min': 0,\n    'reliability_threshold': 0.95,\n}\n\nbpsom_model = BPSOMBertForSequenceClassification.from_pretrained(\n    'bert-base-uncased',\n    config=bpsom_config,\n    bpsom_config=bpsom_model_config\n).to(device)\n\n# Optimizer and scheduler\nbpsom_optimizer = torch.optim.AdamW(bpsom_model.parameters(), lr=learning_rate)\nbpsom_scheduler = get_linear_schedule_with_warmup(\n    bpsom_optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)\n\n# Training loop\nbpsom_history = {\n    'train_loss': [],\n    'train_accuracy': [],\n    'eval_loss': [],\n    'eval_accuracy': [],\n    'som_stats': []\n}\n\nbest_bpsom_acc = 0.0\n\nfor epoch in range(num_epochs):\n    # Reset SOM statistics\n    bpsom_model.bpsom_hidden.som.reset_statistics()\n    bpsom_model.bpsom_hidden.reset_activation_statistics()\n\n    # Train\n    train_metrics = train_epoch(\n        bpsom_model,\n        train_dataloader,\n        bpsom_optimizer,\n        bpsom_scheduler,\n        device,\n        epoch,\n        num_epochs\n    )\n\n    # Update SOM cell labels\n    # print(\"\\nUpdating SOM cell labels...\")\n    bpsom_model.update_som_labels(train_dataloader, device)\n\n    # Get SOM statistics\n    som_stats = bpsom_model.bpsom_hidden.som.get_statistics()\n\n    # Evaluate\n    eval_metrics = eval_epoch(bpsom_model, eval_dataloader, device, \"Validation\")\n\n    # Log\n    print(f\"\\nEpoch {epoch+1}/{num_epochs}:\")\n    print(f\"  Train - Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['accuracy']:.2f}%, \"\n          f\"SOM Usage: {som_stats['som_usage_pct']:.1f}%\")\n    print(f\"  Val   - Loss: {eval_metrics['loss']:.4f}, Acc: {eval_metrics['accuracy']:.2f}%\")\n\n    bpsom_history['train_loss'].append(train_metrics['loss'])\n    bpsom_history['train_accuracy'].append(train_metrics['accuracy'])\n    bpsom_history['eval_loss'].append(eval_metrics['loss'])\n    bpsom_history['eval_accuracy'].append(eval_metrics['accuracy'])\n    bpsom_history['som_stats'].append(som_stats)\n\n    if eval_metrics['accuracy'] > best_bpsom_acc:\n        best_bpsom_acc = eval_metrics['accuracy']\n        print(f\"  ✓ New best validation accuracy!\")\n\n    print(\"-\" * 80)\n\nprint(f\"\\nBP-SOM Training Complete!\")\nprint(f\"Best Validation Accuracy: {best_bpsom_acc:.2f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot BP-SOM results\n",
    "plot_training_curves(bpsom_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize SOM Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize final SOM state\n",
    "print(\"SOM Organization after Training:\")\n",
    "plot_som_visualization(bpsom_model, class_names=['negative', 'positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison plot\n",
    "plot_comparison(baseline_history, bpsom_history)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Baseline BERT:\")\n",
    "print(f\"  Best Validation Accuracy: {max(baseline_history['eval_accuracy']):.2f}%\")\n",
    "print(f\"  Final Validation Loss: {baseline_history['eval_loss'][-1]:.4f}\")\n",
    "print()\n",
    "print(f\"BP-SOM BERT:\")\n",
    "print(f\"  Best Validation Accuracy: {max(bpsom_history['eval_accuracy']):.2f}%\")\n",
    "print(f\"  Final Validation Loss: {bpsom_history['eval_loss'][-1]:.4f}\")\n",
    "print(f\"  SOM Usage: {bpsom_history['som_stats'][-1]['som_usage_pct']:.1f}%\")\n",
    "print()\n",
    "\n",
    "improvement = max(bpsom_history['eval_accuracy']) - max(baseline_history['eval_accuracy'])\n",
    "print(f\"Improvement: {improvement:+.2f}% accuracy\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analyze SOM Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot SOM statistics over epochs\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "som_usage = [stat['som_usage_pct'] for stat in bpsom_history['som_stats']]\n",
    "som_dist = [stat['avg_distance'] for stat in bpsom_history['som_stats']]\n",
    "\n",
    "axes[0].plot(som_usage, marker='o', color='green', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('SOM Usage (%)', fontsize=12)\n",
    "axes[0].set_title('SOM Error Usage Over Time', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(som_dist, marker='o', color='purple', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Average Distance', fontsize=12)\n",
    "axes[1].set_title('SOM Average Distance Over Time', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSOM Statistics:\")\n",
    "print(f\"  Final SOM Usage: {som_usage[-1]:.1f}%\")\n",
    "print(f\"  Final Average Distance: {som_dist[-1]:.3f}\")\n",
    "print(f\"  SOM Grid Size: {bpsom_model.bpsom_hidden.som.grid_size}x{bpsom_model.bpsom_hidden.som.grid_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Models (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save models\n",
    "# torch.save(baseline_model.state_dict(), 'baseline_bert_sst2.pt')\n",
    "# torch.save(bpsom_model.state_dict(), 'bpsom_bert_sst2.pt')\n",
    "# print(\"✓ Models saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates BP-SOM applied to BERT fine-tuning on SST-2:\n",
    "\n",
    "### Key Observations:\n",
    "\n",
    "1. **SOM Organization**: The SOM develops class-specific clusters during training\n",
    "2. **SOM Usage**: The percentage of examples where SOM error is used (reliability >= threshold)\n",
    "3. **Performance**: Compare baseline vs BP-SOM accuracy\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Experiment with different SOM grid sizes\n",
    "- Adjust SOM error weight (α)\n",
    "- Try other GLUE tasks (MRPC, CoLA, etc.)\n",
    "- Implement unit pruning\n",
    "- Analyze learned representations\n",
    "\n",
    "### References:\n",
    "\n",
    "- Weijters, A. (1995). The BP-SOM architecture and learning algorithm. *Neural Processing Letters*, 2:6, pp. 13-16.\n",
    "- Original C implementation: https://github.com/LanguageMachines/bp-som"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}